{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d51a95d0-de07-418d-b7fe-e045716b5aa6",
   "metadata": {},
   "source": [
    "# Homework 2 Continuing to Explore Graph Edge Data\n",
    "---\n",
    "Like Homework 2, below you will find a number of questions with space to place and run your code.\n",
    "\n",
    "Also like Homework 2, several qeustions build upon previous question results, so prgress in a sequential manner, and verifying your results befor proceeding to the next question.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ea0ce06-21ab-427a-9e17-367c8a3663bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "from io import StringIO\n",
    "from functools import partial\n",
    "from typing import Dict, Tuple, List\n",
    "from sortedcontainers import SortedKeyList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ca9782e-dbcd-487a-b09f-0d8697c97ead",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o36.applyModifiableSettings.\n: java.lang.IllegalStateException: LiveListenerBus is stopped.\n\tat org.apache.spark.scheduler.LiveListenerBus.addToQueue(LiveListenerBus.scala:98)\n\tat org.apache.spark.scheduler.LiveListenerBus.addToStatusQueue(LiveListenerBus.scala:81)\n\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:115)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:144)\n\tat org.apache.spark.sql.SparkSession$$Lambda$1117/765451161.apply(Unknown Source)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:144)\n\tat org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:143)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:163)\n\tat org.apache.spark.sql.SparkSession$$Lambda$1116/891629061.apply(Unknown Source)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:161)\n\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:158)\n\tat org.apache.spark.sql.SparkSession$.conf$lzycompute$1(SparkSession.scala:1142)\n\tat org.apache.spark.sql.SparkSession$.conf$1(SparkSession.scala:1142)\n\tat org.apache.spark.sql.SparkSession$.applyModifiableSettings(SparkSession.scala:1145)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:483)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/Users/sadiaakther/Desktop/Big Data Programming/homework3/homework3.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sadiaakther/Desktop/Big%20Data%20Programming/homework3/homework3.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkSession\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sadiaakther/Desktop/Big%20Data%20Programming/homework3/homework3.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m spark \u001b[39m=\u001b[39m (\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sadiaakther/Desktop/Big%20Data%20Programming/homework3/homework3.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     SparkSession\u001b[39m.\u001b[39mbuilder\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sadiaakther/Desktop/Big%20Data%20Programming/homework3/homework3.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m.\u001b[39mmaster(\u001b[39m\"\u001b[39m\u001b[39mspark://127.0.0.1:7077\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sadiaakther/Desktop/Big%20Data%20Programming/homework3/homework3.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# the number of executors this job needs\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sadiaakther/Desktop/Big%20Data%20Programming/homework3/homework3.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m.\u001b[39mconfig(\u001b[39m\"\u001b[39m\u001b[39mspark.executor.instances\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sadiaakther/Desktop/Big%20Data%20Programming/homework3/homework3.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# the number of CPU cores memory this needs from the executor,\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sadiaakther/Desktop/Big%20Data%20Programming/homework3/homework3.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# it would be reserved on the worker\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sadiaakther/Desktop/Big%20Data%20Programming/homework3/homework3.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m.\u001b[39mconfig(\u001b[39m\"\u001b[39m\u001b[39mspark.executor.cores\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sadiaakther/Desktop/Big%20Data%20Programming/homework3/homework3.ipynb#W2sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m.\u001b[39mconfig(\u001b[39m\"\u001b[39m\u001b[39mspark.executor.memory\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m4G\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sadiaakther/Desktop/Big%20Data%20Programming/homework3/homework3.ipynb#W2sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m.\u001b[39mgetOrCreate()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sadiaakther/Desktop/Big%20Data%20Programming/homework3/homework3.ipynb#W2sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sadiaakther/Desktop/Big%20Data%20Programming/homework3/homework3.ipynb#W2sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m sc \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39msparkContext\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyspark/sql/session.py:484\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    480\u001b[0m     session \u001b[39m=\u001b[39m SparkSession(sc, options\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_options)\n\u001b[1;32m    481\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    482\u001b[0m     \u001b[39mgetattr\u001b[39m(\n\u001b[1;32m    483\u001b[0m         \u001b[39mgetattr\u001b[39m(session\u001b[39m.\u001b[39m_jvm, \u001b[39m\"\u001b[39m\u001b[39mSparkSession$\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mMODULE$\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 484\u001b[0m     )\u001b[39m.\u001b[39mapplyModifiableSettings(session\u001b[39m.\u001b[39m_jsparkSession, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_options)\n\u001b[1;32m    485\u001b[0m \u001b[39mreturn\u001b[39;00m session\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[1;32m    170\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o36.applyModifiableSettings.\n: java.lang.IllegalStateException: LiveListenerBus is stopped.\n\tat org.apache.spark.scheduler.LiveListenerBus.addToQueue(LiveListenerBus.scala:98)\n\tat org.apache.spark.scheduler.LiveListenerBus.addToStatusQueue(LiveListenerBus.scala:81)\n\tat org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:115)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:144)\n\tat org.apache.spark.sql.SparkSession$$Lambda$1117/765451161.apply(Unknown Source)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:144)\n\tat org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:143)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:163)\n\tat org.apache.spark.sql.SparkSession$$Lambda$1116/891629061.apply(Unknown Source)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:161)\n\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:158)\n\tat org.apache.spark.sql.SparkSession$.conf$lzycompute$1(SparkSession.scala:1142)\n\tat org.apache.spark.sql.SparkSession$.conf$1(SparkSession.scala:1142)\n\tat org.apache.spark.sql.SparkSession$.applyModifiableSettings(SparkSession.scala:1145)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:483)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"spark://127.0.0.1:7077\")\n",
    "    # the number of executors this job needs\n",
    "    .config(\"spark.executor.instances\", 2)\n",
    "    # the number of CPU cores memory this needs from the executor,\n",
    "    # it would be reserved on the worker\n",
    "    .config(\"spark.executor.cores\", \"2\")\n",
    "    .config(\"spark.executor.memory\", \"4G\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fd2075-86b2-4e30-be7e-6ca15f5818ab",
   "metadata": {},
   "source": [
    "## Question 1: Reading a CSV file and constructing an RDD (5 points)\n",
    "---\n",
    "In this question, like in Homework 2, you will read the `edges_table.csv` file that was included with this assignment. However, this time we will be using the built in text file reading ability of the SparkContext object to read our file.\n",
    "\n",
    "Recall the text file reading will either read each line of the file and return them as entries in an RDD or will read the entire file and return it's name and the file contents as a pair entry in an RDD. Ideally we would like to use the line by line, but our data has a header line in the file which we don't want included in our dataset. So, you will need to use the SparkContext method that will read the whole file, then map the file that was read into a function that will parse the file removing the header, and then placing each of the lines into a resulting RDD with a single record per line.  \n",
    "\n",
    "__NOTE:__ All of the worker processes have to see the file in the same location that you pass in as the file location. If you pass in a relative location to your driver program, I believe that Spark will attach an ablsolute path location to that relative path by adding the path to where your driver is running. This would be useful in case your worker process isn't started from the same directory as your driver program, but I have not confirmed this to be the case, so you may need to play around with the file path information you pass in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad94ef5-faeb-494a-aac7-dddff4222219",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o31.defaultParallelism.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:408)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.lang.Thread.run(Thread.java:745)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:120)\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2559)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:483)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/Users/sadiaakther/Desktop/Big Data Programming/homework3/homework3.ipynb Cell 5\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sadiaakther/Desktop/Big%20Data%20Programming/homework3/homework3.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkContext\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sadiaakther/Desktop/Big%20Data%20Programming/homework3/homework3.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39mgetOrCreate()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sadiaakther/Desktop/Big%20Data%20Programming/homework3/homework3.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m edges_table_rdd \u001b[39m=\u001b[39m sc\u001b[39m.\u001b[39mtextFile(\u001b[39m\"\u001b[39m\u001b[39medges_table.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sadiaakther/Desktop/Big%20Data%20Programming/homework3/homework3.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m edges_header \u001b[39m=\u001b[39m edges_table_rdd\u001b[39m.\u001b[39mfirst()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sadiaakther/Desktop/Big%20Data%20Programming/homework3/homework3.ipynb#W4sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m edges_no_header \u001b[39m=\u001b[39m edges_table_rdd\u001b[39m.\u001b[39mfilter(\u001b[39mlambda\u001b[39;00m line: line \u001b[39m!=\u001b[39m edges_header)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyspark/context.py:986\u001b[0m, in \u001b[0;36mSparkContext.textFile\u001b[0;34m(self, name, minPartitions, use_unicode)\u001b[0m\n\u001b[1;32m    925\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtextFile\u001b[39m(\n\u001b[1;32m    926\u001b[0m     \u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m, minPartitions: Optional[\u001b[39mint\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, use_unicode: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    927\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m RDD[\u001b[39mstr\u001b[39m]:\n\u001b[1;32m    928\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[39m    Read a text file from HDFS, a local file system (available on all\u001b[39;00m\n\u001b[1;32m    930\u001b[0m \u001b[39m    nodes), or any Hadoop-supported file system URI, and return it as an\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[39m    ['aa', 'bb', 'cc', 'x', 'y', 'z']\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 986\u001b[0m     minPartitions \u001b[39m=\u001b[39m minPartitions \u001b[39mor\u001b[39;00m \u001b[39mmin\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaultParallelism, \u001b[39m2\u001b[39m)\n\u001b[1;32m    987\u001b[0m     \u001b[39mreturn\u001b[39;00m RDD(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsc\u001b[39m.\u001b[39mtextFile(name, minPartitions), \u001b[39mself\u001b[39m, UTF8Deserializer(use_unicode))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyspark/context.py:627\u001b[0m, in \u001b[0;36mSparkContext.defaultParallelism\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    616\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefaultParallelism\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m    617\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    618\u001b[0m \u001b[39m    Default level of parallelism to use when not given by user (e.g. for reduce tasks)\u001b[39;00m\n\u001b[1;32m    619\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39m    True\u001b[39;00m\n\u001b[1;32m    626\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 627\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsc\u001b[39m.\u001b[39msc()\u001b[39m.\u001b[39mdefaultParallelism()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[1;32m    170\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o31.defaultParallelism.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:408)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\npy4j.ClientServerConnection.run(ClientServerConnection.java:106)\njava.lang.Thread.run(Thread.java:745)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:120)\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2559)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:483)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "#Place your answer here.\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "edges_table_rdd = sc.textFile(\"edges_table.csv\")\n",
    "\n",
    "edges_header = edges_table_rdd.first()\n",
    "\n",
    "edges_no_header = edges_table_rdd.filter(lambda line: line != edges_header)\n",
    "\n",
    "edges_result = edges_no_header.map(lambda line: line.split(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cffd62-acdd-400a-a5b3-8f14e045fcc1",
   "metadata": {},
   "source": [
    "## Question 2: Constructing Vertex Data Structure RDDs (5 points)\n",
    "---\n",
    "The data that was loaded in Question 1 is just a table of directed edges in some graph.  The first entry of a row is the `from` vertex and the second is the `to` vertex.  \n",
    "\n",
    "What you should do in this question is create an RDD that contains a key/value pair where the key is the `from vertex number` and the value should be a dictionary that contins:\n",
    "  * A list of vertices that the vertex links to named `out_links`\n",
    "  * A list of vertices that link to the vertex indicated by the key called `in_links`\n",
    "\n",
    "Your Homework 2 should already have this answer for you.\n",
    "\n",
    "__NOTE:__ Persist this RDD as it will be used multiple times in later questions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b34645b-f7b1-4132-8566-70b0954ab675",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'edges_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/sadiaakther/Desktop/Big Data Programming/homework3/homework3.ipynb Cell 7\u001b[0m line \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sadiaakther/Desktop/Big%20Data%20Programming/homework3/homework3.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#Place your answer here.\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sadiaakther/Desktop/Big%20Data%20Programming/homework3/homework3.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m pair_rdd \u001b[39m=\u001b[39m edges_result\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m x: (x[\u001b[39m0\u001b[39m], x[\u001b[39m1\u001b[39m]))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sadiaakther/Desktop/Big%20Data%20Programming/homework3/homework3.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Group \"out_links\" and \"in_links\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sadiaakther/Desktop/Big%20Data%20Programming/homework3/homework3.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m out_links \u001b[39m=\u001b[39m pair_rdd\u001b[39m.\u001b[39mgroupByKey()\u001b[39m.\u001b[39mmapValues(\u001b[39mlist\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'edges_result' is not defined"
     ]
    }
   ],
   "source": [
    "#Place your answer here.\n",
    "pair_rdd = edges_result.map(lambda x: (x[0], x[1]))\n",
    "\n",
    "out_links = pair_rdd.groupByKey().mapValues(list)\n",
    "in_links = pair_rdd.map(lambda x: (x[1], x[0])).groupByKey().mapValues(list)\n",
    "\n",
    "vertex_structure_rdd = out_links.leftOuterJoin(in_links).mapValues(lambda x: {'out_links': x[0], 'in_links': x[1] if x[1] else []})\n",
    "\n",
    "vertex_structure_rdd.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a01e98-3d34-4811-b4ae-978a4eb6166e",
   "metadata": {},
   "source": [
    "## Question 3: Write a function to process join tuples (10 points)\n",
    "---\n",
    "In a later question we will be repeatedly joining our result from the previous question with another RDD that contains information we will be updating in an iterative algorithm. This join will be so that we can use the information in both RDDs to update the information in the new RDD. So, in order to do this update process, lets write a function that will process one entry of that join result.\n",
    "\n",
    "Each line in the RDD we will be processing will contain a Tuple with the following format:\n",
    " * Item[0] is an integer indicating which vertex we are processing data for\n",
    " * Item[1] is another Tuple with a Dict in position 0 that is our vertex link information and a float in position 1 that is the current rank of the vertex being processed.\n",
    "\n",
    "We don't care about the current vertex ID so we will ignore this value in this function. We do, however, care about both the rank and the list of vertices this vertex is linked to.  \n",
    "\n",
    "For the output of this function you should return a list of key/value tuples. The key should be the vertex that this vertex has an edge to. The value should be the current rank times `1/N` where `N` is the count of edges going out from this vertex.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f0f7f0-6d09-463c-8dbd-a224ffdf7a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/15 19:58:50 WARN StandaloneAppClient$ClientEndpoint: Drop UnregisterApplication(null) because has not yet connected to master\n"
     ]
    }
   ],
   "source": [
    "#Place your answer here.\n",
    "def rank_order(entry):\n",
    "    vertex_id = entry[0]\n",
    "    data = entry[1]\n",
    "    vertex_info = data[0]\n",
    "    current_rank = data[1]\n",
    "    out_link = vertex_info['out_link']\n",
    "    num_out_links = len(out_link)\n",
    "    rank_order = current_rank / num_out_links if num_out_links > 0 else 0\n",
    "    return [(out_vertex, rank_order) for out_vertex in out_link]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a396adee-e339-418f-8aa1-dd4f121307b8",
   "metadata": {},
   "source": [
    "## Question 4: Calculate Ranks of Vertices (30 points)\n",
    "---\n",
    "In this question you will be performing an iterative update calculation to determine the rank of each vertex in our dataset.  This is accomplisehd by performing the following:\n",
    "\n",
    " 1. Initializing an RDD with key/value pairs, where the key is the vertex in our results from Question 2 and the value is the initial rank value of 1.\n",
    " 2. You then join the rank RDD with our result from Question 2.\n",
    " 3. The result from Step 2 should then be used to calculate the rank contribution values to pass each vertex using our funciton in Question 3.\n",
    " 4. You then reduce (sum) all the rank contributions from Step 3 for each vertex and construct a new rank RDD with the rank value being `0.15+0.85*x` where `x` is the sum of all the contributions for a given vertex calculated in Step 3.\n",
    " 5. Repeat Steps 2 through 4 ten times.  \n",
    "\n",
    "Congratulations, you have completed the page rank algorithm that Google was first developed on! You are now ready to go start your own competing search engine company.\n",
    "\n",
    "__NOTE:__ Once you have completed this algorithm, you should persist the rank results and unpersist the results from Question 2 as we are done with them. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029d02da-c30f-4619-a42d-8b904c97693d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vertex_structure_rdd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/sadiaakther/Desktop/Big Data Programming/homework3/homework3.ipynb Cell 11\u001b[0m line \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sadiaakther/Desktop/Big%20Data%20Programming/homework3/homework3.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#Place your answer here.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sadiaakther/Desktop/Big%20Data%20Programming/homework3/homework3.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m initial_rank \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sadiaakther/Desktop/Big%20Data%20Programming/homework3/homework3.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m ranks \u001b[39m=\u001b[39m vertex_structure_rdd\u001b[39m.\u001b[39mkeys()\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m vertex: (vertex, initial_rank))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sadiaakther/Desktop/Big%20Data%20Programming/homework3/homework3.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m num_iterations \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sadiaakther/Desktop/Big%20Data%20Programming/homework3/homework3.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_iterations):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vertex_structure_rdd' is not defined"
     ]
    }
   ],
   "source": [
    "#Place your answer here.\n",
    "\n",
    "num_iterations = 10\n",
    "\n",
    "initial_rank = 1.0\n",
    "ranks = vertex_structure_rdd.keys().map(lambda vertex: (vertex, initial_rank))\n",
    "\n",
    "def process_vertex_entry(entry):\n",
    "    _, (links, current_rank) = entry  # Ignore the current vertex ID\n",
    "    n = len(links)\n",
    "    redistribution_factor = current_rank / n if n > 0 else 0.0\n",
    "    result = [(link, redistribution_factor) for link in links]\n",
    "    return result\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    contribution = vertex_structure_rdd.join(ranks).flatMap(process_vertex_entry)\n",
    "\n",
    "    ranks = contribution.reduceByKey(lambda x, y: x + y).mapValues(lambda rank: 0.15 + 0.85 * rank)\n",
    "\n",
    "\n",
    "ranks.persist()\n",
    "\n",
    "vertex_structure_rdd.unpersist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c014d13-5e5d-4d19-a4a8-043685791293",
   "metadata": {},
   "source": [
    "## Question 5: Find the Top-k Ranked Vertices (20 points)\n",
    "---\n",
    "\n",
    "Using the methods presented in lecture. Use the results of Question 4 to find the Top 10 ranked vertex IDs in the dataset and print out the ID and rank values in descending order of rank.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a47eee-12d6-439b-8b10-9aa314b13b84",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out_links' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/sadiaakther/Desktop/Big Data Programming/homework3/homework3.ipynb Cell 13\u001b[0m line \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sadiaakther/Desktop/Big%20Data%20Programming/homework3/homework3.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#Place your answer here.\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sadiaakther/Desktop/Big%20Data%20Programming/homework3/homework3.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m vertex_structure_rdd \u001b[39m=\u001b[39m out_links\u001b[39m.\u001b[39mleftOuterJoin(in_links)\u001b[39m.\u001b[39mmapValues(\u001b[39mlambda\u001b[39;00m x: {\u001b[39m'\u001b[39m\u001b[39mout_links\u001b[39m\u001b[39m'\u001b[39m: x[\u001b[39m0\u001b[39m], \u001b[39m'\u001b[39m\u001b[39min_links\u001b[39m\u001b[39m'\u001b[39m: x[\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m x[\u001b[39m1\u001b[39m] \u001b[39melse\u001b[39;00m []})\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sadiaakther/Desktop/Big%20Data%20Programming/homework3/homework3.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m ranks \u001b[39m=\u001b[39m vertex_structure_rdd\u001b[39m.\u001b[39mkeys()\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m vertex: (vertex, initial_rank))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sadiaakther/Desktop/Big%20Data%20Programming/homework3/homework3.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m top_rank \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'out_links' is not defined"
     ]
    }
   ],
   "source": [
    "#Place your answer here.\n",
    "top_rank = 10\n",
    "\n",
    "top_rank_vertices = ranks.sortBy(lambda x: x[1], ascending=False).take(top_rank)\n",
    "\n",
    "for vertex, rank in top_rank_vertices:\n",
    "    print(f\"Vertex ID: {vertex}, Rank: {rank}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
